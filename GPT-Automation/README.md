# GPT just for automation (do instructions with text)

## How to use GPT for automation

For example, the GPT-3 model, which has 175 billion parameters, would require 3.14×1023
FLOPS for training, translating to 355 GPU-years and a cost of USD 4.6 million on a V100 GPU [90]. Memory is another bottleneck; the model’s 175 billion parameters would need 700 GB of memory, far exceeding the capacity of a single GPU


## References 

https://arxiv.org/abs/2307.00150

Examples of gpt source code.

https://github.com/arc53/DocsGPT/tree/main/application

https://github.com/EwingYangs/awesome-open-gpt

https://github.com/korchasa/awesome-chatgpt


Dedicated GPTS

Finance
https://arxiv.org/abs/2303.17564


Paper with good math formula

https://arxiv.org/pdf/2312.08617.pdf
his code of paper
https://github.com/hkust-zhiyao/RTL-Coder